# Podcast Episode Recommendations in a Public Service Setting
This is the repository for my BSc project in Data Science @ IT University of Copenhagen.  

The goal of the project is to implement, analyze and compare the following types of recommenders for podcast episode recommendation:
- [Collaborative Filtering](scripts/05b_cf_recommender.py) (`BiasedMF` from `lenskit.algorithms.als`)
- [Content-based](scripts/05c_cb_recommenders.py)
- [Hybrid recommender (weighted)](scripts/05d_hybrid_recommender.py)

The recommenders are being compared to a popularity-based [baseline recommender](scripts/05a_baseline_recommender.py). The scope of the project is limited to user interactions from [DR LYD](https://www.dr.dk/lyd) (app and web).  

In collaboration with DR (Danmarks Radio). 

Supervisor: [Toine Bogers](https://github.com/toinebogers)


## Project Structure
```
├── data   
│   │
│   ├── episode_descriptions.parquet        <- short descriptions for each episode
│   │
│   ├── episode_metadata.parquet            <- episode metadata
│   │
│   ├── podcast_data_filtered.parquet       <- raw data filtered
│   │
│   ├── podcast_data__int_filtered.parquet  <- intermediary filtered data before applying user filter
│   │
│   ├── podcast_data_raw.parquet            <- raw data
│   │
│   ├── podcast_data_test.parquet           <- test data
│   │
│   ├── podcast_data_train.parquet          <- training data
│   │
│   ├── podcast_data_transformed.parquet    <- filtered data transformed
│   │
│   └── podcast_data_val.parquet            <- validation data
│
├── eda                                   
│   │
│   ├── completion_rate_eda.ipynb           <- exploring the distribution of completion rates and possible transformations
│   │
│   ├── experiments_viz.ipynb               <- visualizing results from hyperparameter experiments
│   │
│   ├── podcast_eda.ipynb                   <- main exploratory data analysis (EDA)
│   │
│   └── split_strategies.ipynb              <- assessing train-test splitting strategies
│
├── embeddings                                   
│   │
│   ├── combined_embeddings.parquet         <- embeddings based on both episode titles and descriptions
│   │
│   ├── descr_embeddings.parquet            <- embeddings based on episode descriptions
│   │
│   └── title_embeddings.parquet            <- embeddings based on episode titles
│
├── experiments         
│   │
│   ├── cb_combi_experiments_results.csv    <- ndcg@10 for different values of the weighting hyperparameter lambda                    
│   │
│   ├── cb_combi_experiments.py             <- hyperparameter tuning for cb combi recommender
│   │
│   ├── cf_experiments_results.csv          <- ndcg@10 for different values of hyperparameters                    
│   │
│   ├── cf_experiments.py                   <- hyperparameter tuning for cf recommender
│   │
│   ├── hybrid_experiments_results.csv      <- ndcg@10 for different values of the weighting hyperparameter lambda
│   │
│   └── hybrid_experiments.py               <- hyperparameter tuning for hybrid recommender
│
├── references                                   
│   │
│   ├── confidentiality_agreement.pdf       <- agreement between me as a student and DR prior to the project 
│   │
│   ├── interview_transcription_dk.txt      <- transcription of interview with Morten Just in Danish (original language)
│   │
│   ├── interview_transcription_eng.txt     <- translated version of interview transcription
│   │
│   └── preliminary_problem_statement.pdf   <- preliminary problem statement from the beginning of the semester
│
├── results      
│   │
│   ├── analysis.ipynb                      <- analysis of results  
│   │
│   ├── cf_optimal.csv                      <- ndcg@10 for each epoch using the optimal hyperparameter values                           
│   │
│   ├── cf_scores_incl_val.parquet          <- scores from cf recommender incl user interactions from validation set
│   │
│   ├── cf_scores.parquet                   <- scores from cf recommender
│   │
│   ├── main_results_analysis.csv           <- p-values from bootstrap analysis of main results
│   │
│   ├── recommendations.json                <- the recommendations generated by each recommender
│   │
│   ├── recommender_eval.json               <- evaluation metrics for each recommender
│   │
│   ├── results_viz.ipynb                   <- visualization of main results
│   │
│   ├── user_eval_2.json                    <- evaluation results at user level @2
│   │
│   ├── user_eval_6.json                    <- evaluation results at user level @6
│   │
│   └── user_eval_10.json                   <- evaluation results at user level @10
│
├── scripts                             
│   │
│   ├── 01_filter.py                        <- filtering the raw data
│   │
│   ├── 02_transform.py                     <- applying data transformations
│   │
│   ├── 03a_extract_metadata.py             <- extracting podcast episode metadata
│   │
│   ├── 03b_extract_embeddings.py           <- extracting embeddings from different textual metadata
│   │
│   ├── 03c_extract_utils.py                <- extracting utilities
│   │
│   ├── 04_train_test.py                    <- splitting the data into a train, validation and test set
│   │
│   ├── 05a_baseline_recommender.py         <- baseline recommender
│   │
│   ├── 05b_cf_recommender.py               <- collaborative filtering recommender
│   │
│   ├── 05c_cb_recommenders.py              <- content-based recommenders
│   │
│   ├── 05d_hybrid_recommender.py           <- hybrid recommender
│   │
│   └── 06_evaluation.py                    <- evaluating recommender systems
│
├── utils  
│   │
│   ├── train_val_interactions.json         <- interactions from train and validation data for each user                        
│   │
│   ├── utils.json                          <- utility dictionaries
│   │
│   └── utils.py                            <- utility functions used in the scripts
│  
├── .gitattributes                          <- for handling large file storage of parquet files
│
├── config.py                               <- configuration file storing variables used in the scripts
│
├── environment.yml                         <- environment file used to create a virtual environment for running the code
│
├── pipeline.py                             <- pipeline to run multiple scripts
│  
└── README.md                               <- project description and how to run the code
```

## How to run the code?
NB! The data folder has been gitignored and it is thus not possible to run the recommenders at the moment. 

It is recommended to run the code using the virtual environment specified by [environment.yml](environment.yml). This requires `conda 24.9.2`.

Open a terminal and run the following commands.

Create the virtual environment by running:
```
conda env create -f environment.yml
``` 

Activate the environment by running:
```
conda activate dr-podcast-recsys
```

To run the full pipeline:
```
python pipeline.py
```

## Hyperparameter tuning
The following scripts are used for hyperparameter tuning and are not a part of [the pipeline](pipeline.py): 
- `experiments/cb_combi_experiments.py`
- `experiments/cf_experiments.py`
- `experiments/hybrid_experiments.py`

To run the [CF experiments](experiments/cf_experiments.py):
```
python experiments/cf_experiments.py --n_comp_vals x_1,x_2,x_3 --damping_vals y_1,y_2,y_3 --reg_vals z_1,z_2,z_3 --bias 1
```
where the `n_comp_vals` must be integers, while `damping_vals` and `reg_vals` are floats. The `bias` argument is an integer equal to 0 or 1 indicating whether or not to train the model with bias terms. When `bias=0` the `damping_vals` does not need to be specified. 

To run the [Content-based experiments](experiments/cb_combi_experiments.py):
```
python experiments/cb_combi_experiments.py --lambda_vals x_1,x_2,x_3 --wght_scheme linear
```
where the `lambda_vals` are floats and the `wght_scheme` is a string equal to `linear` or `inverse`.

To run the [Hybrid experiments](experiments/hybrid_experiments.py):
```
python experiments/hybrid_experiments.py --lambda_vals x_1,x_2,x_3 
```

All possible combinations of the hyperparameter values will be tested and evaluated in terms of `ndcg@10`.  
It is possible to add any desired number of values for each hyperparameter.