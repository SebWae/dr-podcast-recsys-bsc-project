# Podcast Episode Recommendations in a Public Service Setting
This is the repository for my BSc project in Data Science @ IT University of Copenhagen.  

The goal of the project is to implement, analyze and compare the following types of recommenders for podcast episode recommendation:
- [Collaborative Filtering](scripts/05d_cf_recommender.py) (`BiasedMF` from `lenskit.algorithms.als`)
- [Content-based](scripts/05c_cb_recommender.py)
- [Hybrid recommender (weighted)](scripts/05e_hybrid_recommender.py)

The recommenders are being compared to a [popularity-based baseline recommender](scripts/05a_baseline_recommender.py) and are only based on user interactions from [DR LYD](https://www.dr.dk/lyd) (app and web).  

In collaboration with DR (Danmarks Radio). 

Supervisor: [Toine Bogers](https://github.com/toinebogers)


## Project Structure
```
├── data   
│   │
│   ├── episode_descriptions.parquet        <- short descriptions for each episode
│   │
│   ├── episode_metadata.parquet            <- episode metadata
│   │
│   ├── podcast_data_filtered.parquet       <- raw data filtered
│   │
│   ├── podcast_data_raw.parquet            <- raw data
│   │
│   ├── podcast_data_test.parquet           <- test data
│   │
│   ├── podcast_data_train.parquet          <- training data
│   │
│   ├── podcast_data_transformed.parquet    <- filtered data transformed
│   │
│   └── podcast_data_val.parquet            <- validation data
│
├── eda                                   
│   │
│   ├── completion_rate_eda.ipynb           <- exploring the distribution of completion rates and possible transformations
│   │
│   ├── error_analysis.ipynb                <- error analysis of the results
│   │
│   ├── podcast_eda.ipynb                   <- main exploratory data analysis (EDA)
│   │
│   └── split_strategies.ipynb              <- assessing train-test splitting strategies
│
├── embeddings                                   
│   │
│   ├── combined_embeddings.parquet         <- embeddings based on both episode titles and descriptions
│   │
│   ├── descr_embeddings.parquet            <- embeddings based on episode descriptions
│   │
│   └── title_embeddings.parquet            <- embeddings based on episode titles
│
├── references                                   
│   │
│   ├── confidentiality_agreement.pdf       <- agreement between me as a student and DR prior to the project 
│   │
│   ├── interview_transcription_dk.txt      <- transcription of interview with Morten Just in Danish (original language)
│   │
│   ├── interview_transcription_eng.txt     <- translated version of interview transcription
│   │
│   └── preliminary_problem_statement.pdf   <- preliminary problem statement from the beginning of the semester
│
├── results      
│   │
│   ├── cf_experiments.csv                  <- ndcg@10 for different combinations of hyperparameter values
│   │
│   ├── cf_optimal.csv                      <- ndcg@10 for each epoch using the optimal hyperparameter values                           
│   │
│   ├── genre_distributions.csv             <- proportion of recommendations for each genre per recommender
│   │
│   ├── hybrid_experiments.csv              <- ndcg@10 for different values of the weighting hyperparameter lambda
│   │
│   ├── recommendations.json                <- the recommendations generated by each recommender
│   │
│   ├── recommender_eval.json               <- evaluation metrics for each recommender
│   │
│   ├── scores.json                         <- scores from the cf and cb recommenders to be used in the hybrid recommender
│   │
│   └── user_eval.json                      <- evaluation metrics for each user per recommender
│
├── scripts                             
│   │
│   ├── 01_filter.py                        <- filtering the raw data
│   │
│   ├── 02_transform.py                     <- applying data transformations
│   │
│   ├── 03a_extract_metadata.py             <- extracting podcast episode metadata
│   │
│   ├── 03b_extract_embeddings.py           <- extracting embeddings from different textual metadata
│   │
│   ├── 03c_extract_utils.py                <- extracting utilities
│   │
│   ├── 04_train_test.py                    <- splitting the data into a train, validation and test set
│   │
│   ├── 05a_baseline_recommender.py         <- baseline recommender
│   │
│   ├── 05b_cf_experiments.py               <- hyperparameter tuning for cf recommender
│   │
│   ├── 05c_cf_recommender.py               <- collaborative filtering recommender
│   │
│   ├── 05d_cb_recommender.py               <- content-based recommenders
│   │
│   ├── 05e_hybrid_experiments.py           <- hyperparameter tuning for hybrid recommender
│   │
│   ├── 05f_hybrid_recommender.py           <- hybrid recommender
│   │
│   └── 06_evaluation.py                    <- evaluating recommender systems
│
├── utils                             
│   │
│   ├── utils.json                          <- utility dictionaries generated by scripts/03c_extract_utils.py
│   │
│   └── utils.py                            <- utility functions used in the scripts
│  
├── .gitattributes                          <- for handling large file storage of parquet files
│
├── config.py                               <- configuration file storing variables used in the scripts
│
├── environment.yml                         <- environment file used to create a virtual environment for running the code
│
├── pipeline.py                             <- pipeline to run multiple scripts
│  
└── README.md                               <- project description and how to run the code
```

## How to run the code?
It is recommended to run the code using the virtual environment specified by [environment.yml](environment.yml). This requires `conda 24.9.2`.

Open a terminal and run the following commands.

Create the virtual environment by running:
```
conda env create -f environment.yml
``` 

Activate the environment by running:
```
conda activate dr-podcast-recsys
```

To run the full pipeline:
```
python pipeline.py
```

## Hyperparameter tuning
The following scripts are used for hyperparameter tuning and are not a part of [the pipeline](pipeline.py): 
- `05b_cf_experiments.py`
- `05e_hybrid_experiments.py`

To run the [CF experiments](scripts/05b_cf_experiments.py):
```
python scripts/05b_cf_experiments.py --n_comp_vals x_1,x_2,x_3 --damping_vals y_1,y_2,y_3 --reg_vals z_1,z_2,z_3
```
where the `n_comp_vals` must be integers, while `damping_vals` and `reg_vals` are floats. 

Similarly, to run the [Hybrid experiments](scripts/05e_hybrid_experiments.py):
```
python scripts/05e_hybrid_experiments.py --lambda_vals x_1,x_2,x_3
```
where the `lambda_vals` are floats.

All possible combinations of the hyperparameter values will be tested and evaluated in terms of `ndcg@10`.  
It is possible to add any desired number of values for each hyperparameter.